name: Run Scrapers

on:
  schedule:
    # Run every day at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install Python dependencies
      run: |
        cd scrapers
        pip install -r requirements.txt
        
    - name: Install root npm dependencies
      run: npm install
        
    - name: Generate Prisma Clients
      run: npx prisma generate
        
    - name: Run scrapers
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SCRAPER_SECRET: ${{ secrets.SCRAPER_SECRET }}
        VERCEL_URL: ${{ secrets.VERCEL_URL }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        python -m scrapers.cron_handler_db 
        
    - name: Run database cleanup via API
      env:
        VERCEL_URL: ${{ secrets.VERCEL_URL }}
        SCRAPER_SECRET: ${{ secrets.SCRAPER_SECRET }}
      run: |
        curl -X POST "https://$VERCEL_URL/api/admin/cleanup" \
          -H "Content-Type: application/json" \
          -d '{"secret":"'$SCRAPER_SECRET'"}' \
          --fail --show-error
        
    - name: Run duplicate cleanup via API
      env:
        VERCEL_URL: ${{ secrets.VERCEL_URL }}
        SCRAPER_SECRET: ${{ secrets.SCRAPER_SECRET }}
      run: |
        curl -X POST "https://$VERCEL_URL/api/admin/cleanup-duplicates" \
          -H "Authorization: Bearer $SCRAPER_SECRET" \
          --fail --show-error 